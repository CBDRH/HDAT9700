---
title: "HDAT9700: Directed Acyclic Graphs (DAGs)"
tutorial:
  id: "au.edu.unsw.cbdrh.hdat9700.directed.acyclic.graphs"
output:
  learnr::tutorial:
    progressive: false
    allow_skip: true
    css: css/tutorials.css
runtime: shiny_prerendered
description: "HDAT9700: Directed Acyclic Graphs (DAGs)" 
---

![](images/UNSW_2017_Big_Data_landscape.jpg){width="75%"}

```{r setup, include=FALSE, warning=FALSE}
library(learnr)
library(tidyverse)
library(ggplot2)
library(dagitty)
library(ggdag)
library(ggpubr)
library(flipbookr)
library(kableExtra)
knitr::opts_chunk$set(echo = FALSE)


```


## Useful links
http://www.dagitty.net/learn/graphs/table2-fallacy.html
http://www.dagitty.net/learn/graphs/roles.html

Game on terminology
http://www.dagitty.net/learn/graphs/index.html

## Notes from the Harvardx course

"This introductory course to causal diagrams teaches you how to translate expert knowledge into a causal diagrams. By the end of the course you will be able:

To draw causal diagrams under different assumptions
To identify common biases using causal diagrams
To guide data analysis using causal diagrams"


Babies example
- Babies born to mothers who smoked are more likely to die
- BUT among low weight babies, those whose mothers smoked are less likely to die

- "Use simple pictures tho think about causal questions
- Ok answer comes later!


Correlation versus causation
Counderfactuals
"Quantifying casual effects requires the comparison of the same or similar treatments"
"there may be association without causation"
"casusal diagrams represent association and causation simultaneously"


The first lesson of the course introduces you to causal diagrams. You will learn the anatomy of a DAG and the rules of D-separation.

Learning Objectives
After this lesson you should be able to:

1. Identify the features of a causal DAG

2. Understand the rules of d-separation

3. Construct a causal DAG that reflects assumptions of how treatments, outcomes, and other factors relate to one another

4. Distinguish between different structural sources of bias


Estrogen and uterine cancer problem
- Assocaition between estrogen and endometrial cancer observed
- Estrogen use causes uterine bleeding so women went to Drs to investigate, leading to more diagnoses of otherwise unobserved cancers
- 'Referred to as ascertainment bias"
- What about restricting to women who bleed?


 
 
Causal diagrams
- Nodes connected by arrows (or directed edges)
- It is acyclic (can't go in cycles)
- Time goes from left to right; the future can't affect the past
- Causal Markov condition (all common causes included)
- A square box representes conditioning
- DAG for RCT rather than DAG for retrospective cohort study
- DAGs represent causal graphs and statistical models
- "Casusal graphs do not need to include mediators to estimate the effect of A on Y"


Example: smoking, yellow fingers and lung cancer
- A shared common cause between A and Y leads us to expect an association between A and Y, even if A does not cause Y
- e.g. there will be an assocation between yellow fingers and cancer, but no yellow fingers does not cause cancer
- The flow of association between A and Y is blocked when we condition on a common cause L. 

Colliders
- Common effects
- Common effects of A and Y will not induce an association
- Conditioning on a common effect of A and Y will induce an asscoiation between A and Y
- This is known as selection bias
- Similarly, conditioning on something affected by a collider will induce an association between A and Y



Structural sources of association
- cause and effect
- common causes
- conditioning on common effects
Other important source of association, but not structural
- Chance

Causal graph theory
- Descendents (child/grandchild/parent/grandparent)



D (direction) seperation Rules

"Two variables are D-seperated if all paths between them have been blocked

1. "If there are no variables being conditioned on,
a path is blocked if and only if two arrowheads on the path
collide at some variable on the path."

2. "Any path that contains a noncollider that has been conditioned on is blocked"

3. "A collider that has been conditioned on does not block a path."

4. "A collider that has a descendant that has been conditioned on
does not block a path."



Faithfulness
- If the effect of A and Y is opposite in the population it could be the case that although A causes Y, there is no association between A and Y
- "Then we say that the joint distribution of the data is not faithful to the casual DAG"



Confounding
Common causes of exposure and outcome result in confounding bias

Confounding is the bias
Counfounders are variables used to block the backdoor path(s)

Backdoor path 
The backdoor path between a hypotesised cause (A) and outcome (Y) is a path that connects A and Y without using any of the arrows that leave from Y. 

Backdoor path criterion
We can estimate the causal effect of A on Y if we can block all the backdoor path

"To deal with confounding we need to use expert knowledge"

M-bias

Sometimes we won't have the observed confounder so we can use surrogate or proxy confounders

Ways to control for confounding
- These approachesa ssume you can measure all confounders L (not all approaches assume this e.g.)
- stratification
- multivariate regression
- matching
- matching with propensity score
 - inverse probability weighting
 - standardisation (g-formula)
 - g-estimation


"Causal DAGs allow us to identify inconsistencies between our beliefs and our actions"



Limitations
 - can't show interactions
 - can't identify whether causes only apply to some people
 - don't convey numerical information
 
 
## Selection bias
 
 Smoking and dementia example
 
 - For selection to call bias it needs to be realted tot he treatment and the outcome
 
 Example of hormone therapy and cancer:
 - Case control studies, by design, select based on the outcome, e.g. cancer patient more likely to be selected
 - If there is an association between hormone therapy and selection then a bias will be introduced.
 - In this example, controls were selected from women in hospital for hip-fracture (can't run away from interviewer hahaha)
 - But hormone therapy reduces the risk of hip-fracture, therefore there was an arrow from hormone therapy to hip fracture, and as a result an open pathway between hormone therapy and selection. 
 - Controls were less likely to have taken hormone therapy. Therefore, even if there was no association between hip fracture and cancer, the selection would make it seem as if there is. 
 
 
### Follow up studies
 - Eligibility
 - Loss to folow up
 
 - For example of treatment for AIDS - if sicker individuals were more likely to drop out of the study, and sicker individuals were more likely to drop out of the study, this could inuduce a bias for the effect of treatment on AIDS. 
 
## Overview  
Welcome to HDAT9700 Statistical Modelling II - Causal inference and DAGs! 

In this chapter you will start to think about how different models are used based on what sort of questions they are answering. 

***

### Prereadings

The pre-reading for this chapter is a _tweetorial_ by epidemiologist [Ellie Murray](https://twitter.com/EpiEllie) from Boston University School of Public Health. 

The tweetorial focuses on **"the importance of being clear about your question & using that to drive your methods"**. Find it [here](https://twitter.com/EpiEllie/status/1214641734900224003).

Ellie's tweetorial was prompted by a disparaging tweet about [this](https://www.nature.com/articles/bjc2017146.pdf) study, which explores the relationship between frozen shoulder and the risk of cancer. The disparaging tweet criticised the study for failing to adjust for confounding. In her tweetorial, Ellie explains that this was a descriptive study, not one addressing a causal question and that in fact adjusting for confounding may well have led to the wrong answer to the question at hand.

This pre-reading should help you understand that we always need to be clear about the purpose of our models, as our purpose dictates how we specify the model. 


### Core reading:

* [Miguel A. Hernán, John Hsu & Brian Healy (2019) A Second Chance to Get
Causal Inference Right: A Classification of Data Science Tasks, _Chance_, 32(1), 42-49](https://doi.org/10.1080/09332480.2019.1579578)

* A paper on DAGs

### Additional resources: 

* [Shmueli G (2010) To explain or to predict?, _Statistical Science_, 25(3), 289-310](https://projecteuclid.org/euclid.ss/1294167961).
Also, if you haven't already, check out the related [video](https://www.youtube.com/watch?time_continue=1&v=vWH_HNfQVRI&vq=large), linked at the end of the last tutorial.

* [dagitty.net](http://www.dagitty.net/learn/): A great website explaing DAGs and related topics, with built-in applets to test your knowledge

* The Causal Inference Podcast, hosted by Ellie Murray and Lucy D'Agostino McGowan. Check out the first episode: [Talking target trials with Miguel Hernán](https://casualinfer.libsyn.com/casual-inference-talking-target-trials-with-miguel-hernan-episode-01)

## Three tasks of data science

The statisical analyses you will perform as Health Data Scientists can be categorised according to three broad tasks of data science. 

1. Description 
2. Prediction 
3. Causal inference 

**Description** involves using statistical models to summarise the relationship between variables. There is no reliance on any underlying causal theory. 

**Prediction** involves using statistical models to predict new or future outcomes (Y) given a set of input values or variables (X). 

**Causal inference** involves estimating the causal effect of an exposure, treatment or intervention. Answering causal questions boils down to comparing outcomes under two or more scenarios, for example, "Would this patient have better chances of survival given treatment A or Treatment B"?


The table below (from [Hernán et al 2019](https://doi.org/10.1080/09332480.2019.1579578)) provides examples of the types of questions, data and analysis methods associated with the tasks of description, prediction and causal inference. 

![Three tasks of data science (from Hernán et al 2019)](images/hernan-table1.png){width="100%"}


```{r quiz1, echo=FALSE}
quiz(caption = "Quiz: Variation across disciplines ",
  question("**Prediction** has traditionally been associated with which of the following disciplines? (Select all that apply)",
    answer("Finance"),
    answer("Psychology"),
    answer("Bioinformatics", correct = TRUE),
    answer("Education"),
    answer("Environmental science"),
    answer("Natural language processing"),
    answer("Microeconomics"),   
    incorrect = "Prediction is the domain of finance, bioinfomatics and natural language processing. The fields of psychology, education, environmental sciences and microeconnomics are primarily concerned with questions of a causal nature."
  ),
  question("Which modeling tasks are relevant to Health Data Science (Select all that apply)",
    answer("Description", correct = TRUE),
    answer("Prediction", correct = TRUE),
    answer("Causal inference", correct = TRUE),
    incorrect = "All three tasks are relevant to a career in Health Data Science. Machine Learning I (HDAT9500) is primarily concerned with prediction. Topics in Statistical Modelling II (HDAT 9700) touch on description, prediction and causal inference. The important thing is choosing the most relevant approach for the research question."
  )
)
```

<div class="aside">

### "Explanation" or "Causal Inference"?

Causal inference tasks are often referred to as "Explanation" (recall the video at the end of Chapter 1 "To Explain or to Predict?" for example). However, Hernán et al (2019) emphasise that **quantifying causal effects doesn't necessarily equate to explaining causal mechanisms**. This is a fair point: knowing that smoking causes cancer is different to knowing which chemicals are carcinogenic, for example. 

</div>

 

## Variable selection
As you learned from this chapter's pre-reading, how we go about choosing the variables to include in the model depend crucially on what we are trying to achieve.

For **predictive** models, the goal is to build a model that accurately predicts the outcome of interest with high sensitivity and specificity, and performs well on new data. In terms of variable selection, the primary consideration is choosing variables that help to achieve this goal (although you would also consider practical things like what variables will be available at the time of prediction). Variable selection could be based on univariate analysis, selecting variables that show a strong association with the outcome, or a selection algorithm like forward, backward or stepwise selection. Many machine learning algorithms will will automatically select (or appropriately weight) the most predictive variables from those available. The covariates in a predictive model all play the same role so we can refer to them collectively as **predictors**.

For **causal** models, the goal is to estimate the effect of a given intervention or treatment on a predefined outcome of interest. Really, the only parameter of interest from our causal model is the estimate for that intervention or treatment. In this context, other variables (that may or may not be available in your dataset) have different roles and different names which you might already have come across: **confounders**, **mediators** and **colliders**. Importantly, this means that **just because a variable is correlated with the outcome of interest, that doesn't necessarily mean it is a good candidate to include in the model**. In order to understand what variables to include in a causal model, we need to recognise these different types of variables, and understand the role they play in the causal relationship of interest. 

Distinguishing between confounders, mediators, colliders and other types of variables relies on expert knowledge about the subject matter in question. There is no  algorithm or automated selection procedure that can replace this knowledge. Directed Acyclic Graphs, the main topic of this chapter, are an important tool that help us to formalise and graphically represent expert knowledge, and in doing so can help us to select the variables that should be included in a causal model.

## DAGS 

### Directed Acyclic Graphs (DAGs) 

DAGs are graphical tools used to represent the assumptions about causal relationships for a given problem. Representing these assumptions using a graph is an important approach because it allows you to 

* Express expert knowledge explicitly 
* Facilitate discussions about variable selection with co-authors 
* Communicate your assumptions to readers 

### What is a DAG 
DAGs are diagrams comprising nodes (circles or squares) connected by edges (arrows). These diagrams represent assumptions about the causal relationships of interest. The nodes in a DAG represent observed or unobserved variables or constructs while the arrows indicate the direction of the causal relationship between variables. 

In this simple DAG, the arrow points from X to Y, representing the asumption that **X** causes **Y**.

```{r}
dag1 <- dagify(Y ~ X) %>% ggdag() + theme_dag()
dag1
```

The absense of an arrow between two variables represents the assumption that neither neither variable is causing or caused by the other. In the next DAG, **Z** causes **X** and **Z** causes **Y**, but **X** and **Y** are _causally_ unrelated (there is no arrow pointing from **X** to **Y** or from **Y** to X). 

```{r}
ggdag_confounder_triangle() +
  theme_dag()
```

As we will discuss at length, the absense of an arrow between two nodes on a DAG, doesn't mean that the variables are unassociated in an observational dataset. What if our DAG above represented **sunshine**, **icecream sales** and **sunburn**?

```{r}
ggdag_confounder_triangle(x="icecream sales", y="sunburn", z="sunshine", 
                          use_labels = "label",
                          label_col = "black") +
  theme_dag()
```

We know that icecream sales definitely don't cause sunburn, but you wouldn't be surprised to find an association between icecream sales and incidence of sunburn (a case of correlation not implying causation!). This is a classic example of **confounding bias**. The relationship between icecream sales and sunburn is **confounded** by sunshine, inducing an association between the two variables, even though there is no causal relationship. 

By using expert knowledge to express our qualitative understanding of causal relationships of interest, DAGs help us to distinguish between association and causation.    


### Properties of DAGS
There are some formal requirements for a causal DAG. As indicated by the name, DAGs should be **directed** and **acyclic**.

* **Directed** The relationship between nodes are _directional_. The arrows indicate the direction of causality.

* **Acyclic** DAGs must not have _cycles_ between the nodes, i.e. a variable should not "cause" itself, either directly or through other variables. You can spot a cyclic relationship if it is possible to start at one node and follow the directional arrows to arrive back at the same node. 

Below is an example of a **cyclic** graph. The logic here is tempting: good health will improve educational outcomes, high levels of education will increase income and higher income can improve health. However this results in a cyclic graph: it is posible to start at any node and follow the directional arrows to return to that node, for example the path **health** $\rightarrow$ **education** $\rightarrow$ **income** $\rightarrow$ **health**.

```{r}
dag2 <- dagify(Y ~ X, Z ~ Y, X ~ Z, labels = c(X="health", Y="education", Z="income"))
ggdag(dag2, use_labels = "label") +
  theme_dag()
```

This would suggest that health status causes health status. Because this graph is cyclic and not acyclic, it does not meet the formal requirements for a DAG. Often, we can untangle cyclic relationships by considering the timing of causes and effects. For example, the relationship between health, education and income could be re-expressed more meaningfully by considering health status at different points in time, as represented below.


```{r}

coords <- list(
  x = c(X1 = 0, Y = 2, Z = 4, X2 = 6),
  y = c(X1 = 1, Y = 0, Z = 0, X2 = -1)
)

dag3 <- dagify(Y ~ X1, Z ~ Y, X2 ~ Z,
                labels = c(X1="health in childhood",
                            Y="education",
                            Z="income",
                            X2="health in adulthood"),
                coords = coords)
ggdag(dag3, use_labels = "label") +
  theme_dag()
```

This is a now valid DAG because it is **directional** and **acyclic**. Note the DAG above follows the convention that time moves from left to right. 

### The causal Markov assumption
A further property of a causal DAG is the **causal Markov assumption**. Although this sounds a bit scary, it is quite simple (cf Hernán and Robins 2020 Chapter 6 Technical Point 6.1 for a formal definition). It means that a common cause of any pair of variables on a DAG should also be included in the DAG.

As an example, let's say we are interested in the causal relationship between between aspirin and pulmonary embolism, in a setting where aspirin is more often prescribed to patients with high blood pressure. In this case, high blood pressure is a **common cause** of both aspirin usage and pulmonary embolism: patients with high blood pressure are more likely to be taking aspirin and high blood pressure increases the risk of a pulmonary embolism. To satisfy the causal Markov assumption, blood pressure should be included in the DAG.

``` {r}
ggdag_confounder_triangle(x="aspirin", y="pulmonary embolism", z="blood pressure", 
                          use_labels = "label",
                          label_col = "black",
                          text = FALSE,
                          x_y_associated = TRUE) +
  theme_dag()
```


Now, imagine a different setting where aspirin has been administered as the intervention in a randomised control trial. In this case, patients are randomly assigned, so there is no causal relationship between blood pressure and taking aspirin. Blood pressure still increases the risk of having a pulmonary embolism, but it is no longer a **common cause** of taking aspirin and having a pulmonary embolism so it is not necessary to include on the DAG. 

```{r}
coords <- list(
  x = c(x = 0, y = 1),
  y = c(x = 1, y = 1)
)

dag4 <- dagify(y ~ x,
                labels = c(x="aspirin", y="pulmonary embolism"),
                coords = coords)

ggdag(dag4, use_labels = "label", text = FALSE) +
  theme_dag()
```

*** 

### Other terminology

Consider the DAG below which an investigator might use when postulating the causal relationship between exercise and melanoma. We will use this DAG to explain some additional useful terminology.

``` {r}
coords <- list(
  x = c(a = 2, b = 1, c = 3, d = 5, e = 7),
  y = c(a = 2, b = 1.1, c = 1, d = 1, e = 1.5)
)

dag5 <- dagify(b ~ a,
               c ~ b,
               d ~ c,
               e ~ d,
               e ~ a,
               labels = c(a="health consciousness",
                          b="exercise",
                          c="sun exposure",
                          d="melanoma",
                          e="screening"),
               exposure = "b",
               outcome = "d",
               coords = coords) 
  ggdag(dag5, use_labels = "label", text = FALSE) + 
theme_dag()
```

#### Path
A path between two nodes in a DAG is a route that connects the two nodes, visiting no node more than once. The arrows don't have to point in the same direction, but if they do the path is considered a _causal path_. 

Below are examples of paths:

* exercise $\leftarrow$ health consciousness $\rightarrow$ screening (Not a causal path) 
* health consciousness $\rightarrow$ screening  $\leftarrow$ melanoma (Not a causal path) 

Below are examples of **causal** paths 

* exercise $\rightarrow$ sun exposure $\rightarrow$ melanoma
* sunexposure $\rightarrow$ melanoma 

#### Parent
The parents for a given node in a DAG are the set of variables with an arrow pointing directly into that node. For example, the parent of **melanoma** is **sun exposure**. 

#### Child
A child of a given node X is any variable with an arrow pointing directly from X. For example, **exercise** and **screening** are child nodes of **health consciousness**. 

#### Descendants and Ancestors
The ancestors of a node are any nodes on the causal path leading to that node. The descendants of a node are any nodes that can be reached by following causal paths. 

* The **ancestors** of sun exposure are exercise and health consciousness 
* The **descendants** of health consciousness are exercise, sun exposure, melanoma and screening


#### Confounder 
A confounder is a **common cause** of two nodes (usually the common cause of an exposure and an outcome)
* Health consciousness is a confounder of exercise and screening

#### Mediator
A mediator is a node that lies on the causal path between two other nodes.

* Sun exposure is a mediator on the path between exercise and melanoma 

#### Collider 
A collider is a node with two directional arrows pointing towards it. 

* Screening is a collider on the path health consciousness $\rightarrow$ screening $\leftarrow$ melanoma



## Quiz

```{r, out.width = '100%'}

a <- dagify(Z ~ X, Y ~ X) %>% ggdag(node_size = 6) + theme_dag() + border() + labs(title = "(A)")
b <- dagify(Y ~ X, X ~ Z, Z ~ Y) %>% ggdag(node_size = 6) + theme_dag() + border() + labs(title = "(B)")
c <- dagify(Y ~ X) %>% ggdag(node_size = 6) + theme_dag() + border() + labs(title = "(C)")

ggarrange(a, b, c, nrow = 1)

```

```{r quiz2, echo=FALSE}
quiz(caption = "Quiz: DAGS",
  question("Which of the diagrams above is not a valid DAG?",
    answer("A"),
    answer("B", correct = TRUE, message = "Correct, (B) is not a valid DAG because it is not _directional_. This DAG would suggest that **X** causes **Y** and **Y** causes **X**."),
    answer("C"),
    incorrect = "This is a valid DAG, try again!",
    allow_retry = TRUE
  )
)
```

*** 

```{r, out.width = '100%'}
ggdag_confounder_triangle(x="Maternal age", y="Low birthweight", z="Social deprivation", 
                          use_labels = "label",
                          label_col = "navy",
                          text = FALSE,
                          x_y_associated = TRUE) +
  theme_dag()
```

```{r quiz3, echo=FALSE}
quiz(caption = "Quiz: DAGS",
     question("Which of the following statements matches the DAG above?",
              answer("Maternal age confounds the relationship between social deprivation and low birth weight."),
              answer("Low birthweight confounds the relationship between social deprivation and maternal age."),
              answer("Social deprivation confounds the relationship between maternal age and low birthweight", correct = TRUE, message = "Correct. This DAG suggests that social deprivation is a common cause of maternal age and low birthweight"),
              incorrect = "Not quite right, try again!",
              allow_retry = TRUE
     )
)
```

#### Want more? 
Try testing yourself on DAG terminology using this applet on [dagitty.net](http://www.dagitty.net/learn/graphs/index.html).



## Confounding variables

### Recall the definition
Confounding arises when there is a variable **Z** that influences the exposure of interest **X** _and_ the outcome of interest **Y**. We say that the relationship between **X** and **Y** is _confounded_ and that **Z** is the _confounder_.

Depending on the discipline, confounding is also referred to as a third variable or a lurking variable. 


### Example
The relationship between mother's age at birth (exposure) and child's birth weight (outcome) may be confounded by mother's socioeconomic status.    


### How does this look in a DAG?

```{r}
ggdag_confounder_triangle(x="Maternal age", y="Birth weight", z="Socioeconomic status", 
                          use_labels = "label",
                          label_col = "black") +
  geom_dag_edges_link(arrow = grid::arrow()) + 
  theme_dag()
```



## Mediating variables

### Definition
A mediating variable **M** is a variable that lies on the causal pathway between an exposure **X** and outcome **Y**.

### Example
A researcher might hypothesise that quality of sleep is a mediating variable between exercise (exposure) and mood (outcome). 

### How does this look in a DAG?

```{r}
ggdag_mediation_triangle(x="Exercise", y="Mood", m="Sleep quality", 
                          use_labels = "label",
                          label_col = "black") +
  geom_dag_edges_link(arrow = grid::arrow(type = 'closed')) + 
  theme_dag()
```

*** 

<div class="aside">

### Muddling mediation

Mediation often gets confused with two closely related epidemiological concepts, _interaction_ and _effect modification_. Below is a quick summary distinguishing these three terms, adapted from the paper [Corraini, Priscila, et al. "Effect modification, interaction and mediation: an overview of theoretical insights for clinical investigators." Clinical epidemiology 9 (2017): 331](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5476432/)

The table below provides a quick summary of these three concepts. If you come across these terms and need a reminder please check out the paper!

</div>

```{r}
tab <- data.frame(
  type = c("Effect modification",
            "Interaction",
            "Modification"),
  
  aim = c("Separate exposure effects according to another variable",
          "Evaluate individual and joint effects of exposures",
          "Evaluate direct and indirect effects of exposures"),
  
  question = c("Does the new drug have a different effect for men compared to women?",
               "Does drug A work better if the patient is also taking drug B?",
               "Does exercise improve mood directly, or does exercise improve sleep and then better sleep results in improved mood?") 
  
)


tab %>% 
  knitr::kable(col.names = c("Type of assessment", "Aim of the assessment", "Example question")) %>% 
  kable_styling()
```




## Collider variables

### Definition
A collider is a common effect of two variables. 

### Example
Suppose a researcher is interested in whether respiratory syncytial virus infection (exposure) during infancy causes asthma in early childhood (outcome). In this case, admissions to the Emergency Department (ED) during early childhood would be a potential collider, because both RSV and asthma will increase the probability of ED admission during early childhood.  

### How does this look in a DAG?
```{r}
ggdag_collider_triangle(x="RSV infection", y="Asthma", m="ED admission", 
                          use_labels = "label",
                          label_col = "black") +
  geom_dag_edges_link(arrow = grid::arrow(type = 'closed')) + 
  theme_dag()
```

As we will see, identifying colliders is important because controlling for a collider can introduce bias. To continue the above example, controlling for the number of ED admissions during early childhood would actually _**bias**_ estimates of the relationship between RSV infection and asthma.


<div class="shiny">

### Explore this further
You can explore this example and the concept of collider bias further using this [interactive Shiny app](https://cbdrh.shinyapps.io/collider-bias/). The app can also be launched by entering `hdat9700tutorials::run('collider-bias')` at the console. 

</div>


## D-separation

Now that you know how to represent and recognise different types of relationships in a DAG, how can a DAG help us to choose what variables to include in a causal model? The answer lies in the concept of **d-separation**. 

**Definition:** Two variables in a DAG are said to be d-separated if there are no open backdoor paths between them.

This definition introduces two new concepts that we also have to define: _backdoor_ paths, and what makes an _open_ path.

#### 1. Backdoor paths
**Definition:** A backdoor path between X and Y is a path that leaves through a parent of X and points to Y.

For example, the path **icecream sales** $\leftarrow$ **sunshine** $\rightarrow$ **sunburn** is a backdoor path between icecream sales and sunburn.
```{r}
ggdag_confounder_triangle(x="icecream sales", y="sunburn", z="sunshine", 
                          use_labels = "label",
                          label_col = "black") +
  theme_dag()
```

You will recognise this as an example of confounding. In this case, the presence of sunshine induces a correlation or _dependence_ between icecream and sunburn. The "d" in d-connected stands for **dependence**! 


#### 2. Open paths

A path between two nodes X and Y is open if the path carries information or dependence between X and Y. We can open or close a path between two nodes by controlling for an intermediate variables. In most contexts, conditioning will mean stratifying by a variable or adding the variable to a regression model.

Importantly, however, whether or not controlling for an intermediate variable Z opens the the pathway or closes the pathway depends on the role of Z. The table below outlines what happens in three different scenarios.


```{r}
tab <- data.frame(
  
  A = c("![](/images/mediator-dag.png){width='80%'}",
    "![](/images/confounder-dag.png){width='80%'}",
    "![](/images/collider-dag.png){width='80%'}"
  ),
  
  B = c("Mediator",
            "Confounder",
            "Collider"),
  
  C = c("Path between X and Y is OPEN",
          "Path between X and Y is OPEN",
          "Path between X and Y is CLOSED"),
  
  D = c("CLOSES path between X and Y.",
          "CLOSES path between X and Y",
          "OPENS path between X and Y")
  
)


tab %>% 
  knitr::kable(col.names = c("DAG", "Role of Z", "Status before controlling for Z", "Effect of controlling for Z")) %>% 
  kable_styling() %>% 
  column_spec(1, width = "4cm") %>% 
  column_spec(2, width = "2cm") %>% 
  column_spec(3, width = "3cm")
```

Note that conditioning on a mediator or a confounder will CLOSE the path between X and Y. In the case of colliders, the path is closed unless we condition on Z in which case we OPEN the path between X and Y.


### How does this all help?

Having read through the section above, you should be comfortable with the idea of: 

1. A backdoor path 
2. An open or closed path

Now, recall the definition of d-separation: 

> Two variables in a DAG are said to be d-separated if there are no open backdoor paths between them 


<div class="under-the-bonnet">

### Apply the concept of d-separation to inform variable selection in causal models

In order to estimate the causal effect of an exposure X on an outcome Y, we have to make sure that X and Y are d-separated. That is, there can be no open backdoor paths between X and Y. 

</div>





## Quiz

## Selection Bias 

## Quiz

## M-Bias
Tweetorial from Dr Ellie Murray ([@EpiEllie](twitter.com/EpiEllie)) focuses on **"applying conclusions about causation to results obtained via methods designed only for finding correlations"**. This example shows a real-world example of M-bias in play, which erroneously led some to conclude that being a current smoker was protective of COVID-19 hospital deaths. Find it [here](https://twitter.com/EpiEllie/status/1258607277357006849)

## Exercise
Draw DAG consistent with two different hypotheses and estimate the corresponding model 


## Extra: Coding DAGs in R

### Coding DAGS in R: Overview 
The DAGs in this tutorial are created using the `ggdag` package. If you are interested in learning more about how to draw DAGs in this way, check out the vignette on the package [here](https://cran.r-project.org/web/packages/ggdag/vignettes/intro-to-ggdag.html).


### Demonstration
Navigate through the slides below to see an example of `ggdag` in action. 

```{r, out.width='100%', out.height='100%'}
knitr::include_url("images/flipbook-ggdag.html")
```

Note that:

* By default, the position of nodes is random. You can specify fixed node positions using `coords = `. 
* The funtion `dagify()` is used to define the relationships between nodes. E.g. `y ~ x` indicates that an arrow will point from **x** to **y**.  
* Passing the output of `dagify()` to `node_parents(x)` creates a dataframe distinguishing between child and parent nodes of **x**. 
* `geom_dag_point()` and `geom_dag_edges()` plot the results. 
* Applying `theme_dag()` provides a nice clean plot. 
 
 
### Try it yourself
Try experimenting with the code below to create your own DAG.

```{r ggdag1, exercise = TRUE, exercise.lines = 25}
# specifying fixed coordinates means that the nodes of the DAG will 
# always be in the same in the same place. If we don't include this, 
# the nodes will be randomly arranged each time the DAG is drawn
coords <- list(
        x = c(x = 0, y = 1, a = 1, b = 0),
        y = c(x = 0, y = 1, a = 0, b = 1)
    )


dagify(y ~ x + a + b,
       x ~ a + b,
       exposure = "x",
       outcome = "y",
       coords = coords) %>%  
  node_parents("x") %>% 
  ggplot(aes(x=x, y=y, xend=xend, yend=yend, color=parent)) + 
  geom_dag_point() + 
  geom_dag_edges() +
  geom_dag_text(col = "white", size = 6) + 
  geom_dag_edges_link(arrow = grid::arrow()) + 
  theme_dag() + 
  scale_color_hue(
    breaks  = c("parent", "child")
    ) 
```
 
You might notice that this code differs slightly from the code in the flipbook slides above. The flipbook code uses _slow ggplot_ as described by the `flipbookr` package developer Evangeline Reynolds in her [blog](https://evangelinereynolds.netlify.app/post/slow-ggplot/). Slow ggplot is an approach to writing ggplot code that focuses on incremental changes, for example, instead of specifying `aes(x=xy, y=y, xend=xend, yend=yend, color=parent)` in a single line, slow ggplot separates out each aesthetic element to a separate line, to emphasise how that element affects the graph. The editable code above dispenses with the slow ggplot syntax, and is more typical of how you will write R code in the wild.    
 
 
## Summary 
 
 
 
 
