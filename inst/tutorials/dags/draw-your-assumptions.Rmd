---
title: "HDAT9700: Draw your assumptions before you draw your conclusions"
tutorial:
  id: "au.edu.unsw.cbdrh.hdat9700.tutorials.draw.your.assumptions"
output:
  learnr::tutorial:
    progressive: false
    allow_skip: true
    css: css/tutorials.css
runtime: shiny_prerendered
---

![](images/UNSW_2017_Big_Data_landscape.jpg){width="75%"}

```{r setup, include=FALSE, warning=FALSE}
library(learnr)
library(dagitty)
library(ggdag)
knitr::opts_chunk$set(echo = FALSE)


```


## Notes from the Harvardx course

"This introductory course to causal diagrams teaches you how to translate expert knowledge into a causal diagrams. By the end of the course you will be able:

To draw causal diagrams under different assumptions
To identify common biases using causal diagrams
To guide data analysis using causal diagrams"


Babies example
- Babies born to mothers who smoked are more likely to die
- BUT among low weight babies, those whose mothers smoked are less likely to die

- "Use simple pictures tho think about causal questions
- Ok answer comes later!


Correlation versus causation
Counderfactuals
"Quantifying casual effects requires the comparison of the same or similar treatments"
"there may be association without causation"
"casusal diagrams represent association and causation simultaneously"


The first lesson of the course introduces you to causal diagrams. You will learn the anatomy of a DAG and the rules of D-separation.

Learning Objectives
After this lesson you should be able to:

1. Identify the features of a causal DAG

2. Understand the rules of d-separation

3. Construct a causal DAG that reflects assumptions of how treatments, outcomes, and other factors relate to one another

4. Distinguish between different structural sources of bias


Estrogen and uterine cancer problem
- Assocaition between estrogen and endometrial cancer observed
- Estrogen use causes uterine bleeding so women went to Drs to investigate, leading to more diagnoses of otherwise unobserved cancers
- 'Referred to as ascertainment bias"
- What about restricting to women who bleed?


 
 
Causal diagrams
- Nodes connected by arrows (or directed edges)
- It is acyclic (can't go in cycles)
- Time goes from left to right; the future can't affect the past
- Causal Markov condition (all common causes included)
- A square box representes conditioning
- DAG for RCT rather than DAG for retrospective cohort study
- DAGs represent causal graphs and statistical models
- "Casusal graphs do not need to include mediators to estimate the effect of A on Y"


Example: smoking, yellow fingers and lung cancer
- A shared common cause between A and Y leads us to expect an association between A and Y, even if A does not cause Y
- e.g. there will be an assocation between yellow fingers and cancer, but no yellow fingers does not cause cancer
- The flow of association between A and Y is blocked when we condition on a common cause L. 

Colliders
- Common effects
- Common effects of A and Y will not induce an association
- Conditioning on a common effect of A and Y will induce an asscoiation between A and Y
- This is known as selection bias
- Similarly, conditioning on something affected by a collider will induce an association between A and Y



Structural sources of association
- cause and effect
- common causes
- conditioning on common effects
Other important source of association, but not structural
- Chance

Causal graph theory
- Descendents (child/grandchild/parent/grandparent)



D (direction) seperation Rules

"Two variables are D-seperated if all paths between them have been blocked

1. "If there are no variables being conditioned on,
a path is blocked if and only if two arrowheads on the path
collide at some variable on the path."

2. "Any path that contains a noncollider that has been conditioned on is blocked"

3. "A collider that has been conditioned on does not block a path."

4. "A collider that has a descendant that has been conditioned on
does not block a path."



Faithfulness
- If the effect of A and Y is opposite in the population it could be the case that although A causes Y, there is no association between A and Y
- "Then we say that the joint distribution of the data is not faithful to the casual DAG"



Confounding
Common causes of exposure and outcome result in confounding bias

Confounding is the bias
Counfounders are variables used to block the backdoor path(s)

Backdoor path 
The backdoor path between a hypotesised cause (A) and outcome (Y) is a path that connects A and Y without using any of the arrows that leave from Y. 

Backdoor path criterion
We can estimate the causal effect of A on Y if we can block all the backdoor path

"To deal with confounding we need to use expert knowledge"

M-bias

Sometimes we won't have the observed confounder so we can use surrogate or proxy confounders

Ways to control for confounding
- These approachesa ssume you can measure all confounders L (not all approaches assume this e.g.)
- stratification
- multivariate regression
- matching
- matching with propensity score
 - inverse probability weighting
 - standardisation (g-formula)
 - g-estimation


"Causal DAGs allow us to identify inconsistencies between our beliefs and our actions"



Limitations
 - can't show interactions
 - can't identify whether causes only apply to some people
 - don't convey numerical information
 
 
## Selection bias
 
 Smoking and dementia example
 
 - For selection to call bias it needs to be realted tot he treatment and the outcome
 
 Example of hormone therapy and cancer:
 - Case control studies, by design, select based on the outcome, e.g. cancer patient more likely to be selected
 - If there is an association between hormone therapy and selection then a bias will be introduced.
 - In this example, controls were selected from women in hospital for hip-fracture (can't run away from interviewer hahaha)
 - But hormone therapy reduces the risk of hip-fracture, therefore there was an arrow from hormone therapy to hip fracture, and as a result an open pathway between hormone therapy and selection. 
 - Controls were less likely to have taken hormone therapy. Therefore, even if there was no association between hip fracture and cancer, the selection would make it seem as if there is. 
 
 
### Follow up studies
 - Eligibility
 - Loss to folow up
 
 - For example of treatment for AIDS - if sicker individuals were more likely to drop out of the study, and sicker individuals were more likely to drop out of the study, this could inuduce a bias for the effect of treatment on AIDS. 
 
## Overview  
Welcome to HDAT9700 Statistical Modelling II - Causal inference and DAGs! 

In this chapter you will start to think about how different models are used based on what sort of questions they are answering. 

***

### Core reading:

* [Miguel A. Hernán, John Hsu & Brian Healy (2019) A Second Chance to Get
Causal Inference Right: A Classification of Data Science Tasks, _Chance_, 32(1), 42-49](https://doi.org/10.1080/09332480.2019.1579578)

* A paper on DAGs

### Additional reading: 

[Shmueli G (2010) To explain or to predict?, _Statistical Science_, 25(3), 289-310](https://projecteuclid.org/euclid.ss/1294167961)
* Maybe the epi podcast? 
* EdX course? 

## Three tasks of data science

The statisical analyses you will perform as Health Data Scientists can be categorised according to three broad tasks of data science. 

1. Description 
2. Prediction 
3. Causal inference 

**Description** involves using statistical models to summarise the relationship between variables. There is no reliance on any underlying causal theory. 

**Prediction** involves using statistical models to predict new or future outcomes (Y) given a set of input values or variables (X). 

**Causal inference** involves estimating the causal effect of an exposure, treatment or intervention. Answering causal questions bolis down to comparing outcomes under two or more scenarios: "Would I have 

uses statistical models to estimate what would have happened under alternative (often hypothetical) scenarios. For example, "Would this patient 


The table below (from [Hernán et al 2019](https://doi.org/10.1080/09332480.2019.1579578)](images/hernan-table1.png){width="75%"}) provides examples of the types of questions, data and analytic questions associated with the tasks of description, prediction and causal inference. 

![Three tasks of data science (from Hernán et al 2019)](images/hernan-table1.png){width="75%"}


```{r quiz1, echo=FALSE}
quiz(caption = "Quiz: Variation across disciplines ",
  question("**Prediction** has traditionally been associated with which of the following disciplines? (Select all that apply)",
    answer("Finance"),
    answer("Psychology"),
    answer("Bioinformatics", correct = TRUE),
    answer("Education"),
    answer("Environmental science"),
    answer("Natural language processing"),
    answer("Microeconomics"),   
    incorrect = "Prediction is the domain of finance, bioinfomatics and natural language processing. The fields of psychology, education, environmental sciences and microeconnomics are primarily concerned with questions of a causal nature."
  ),
  question("Which modeling tasks are relevant to Health Data Science (Select all that apply)",
    answer("Description", correct = TRUE),
    answer("Prediction", correct = TRUE),
    answer("Causal inference", correct = TRUE),
    incorrect = "All three tasks are relevant to a career in Health Data Science. Machine Learning I (HDAT9500) is primarily concerned with prediction. Topics in Statistical Modelling II (HDAT 9700) touch on description, prediction and causal inference. The important thing is choosing the most relevant approach for the research question."
  )
)
```

The key difference is the role of expert knowledge


<div class="aside">

### "Explanation" or "Causal Inference"?

Causal inference tasks are often referred to as "Explanation" (recall the video at the end of Chapter 1 "To Explain or to Predict?" for example). However, Hernán et al (2019) emphasise that **quantifying causal effects doesn't neccesarily equate to explaining causal mechanisms**. This is a fair point: knowing that smoking causes cancer is different to knowing which chemicals are carcinogenic, for example. 

</div>

 
 
 
 
## Small group exercise

Consider the following four papers. 

1. a 
2. b 
3. c 
4. d 

Discuss each paper with your table. Do you think the primary modelling task is description, prediction or causal inference? 

Consider the following questions: 
* What is the main dependent variable or outcome? 
* What is the main indepependent variable or exposure? 
* What other variables are used in the analysis and what is their role? 
* How would you express the research question?
 
## Variable selection
Mention of common selection strategies 

## Group exercise 
Given a dataset with a known (but not revealed) data generating mechanism, students estimate causal effect of preterm birth on early development. share results 


## DAGS 

### Directed Acyclic Graphs (DAGs) 

DAGs are graphical tools used to represent the assumptions about causal relationships for a given problem. Visualising using a graph is an important approach because it allows you to 

* Express expert knowledge explicitly 
* Facilitate discussions about variable selection with coauthors 
* Communicate your assumptions to readers 

### What is a DAG 
DAGs are diagrams comprising nodes (circles or squares) connected by edges (arrows). These diagrams represent assumptions about the causal relationships of interest. The nodes in a DAG represent observed or unobserved variables or constructs while the arrows indicate the direction of the causal relationship between variables. 

The simple DAG below the arrow points from X to Y, representing the asumption that **Z** causes **Y**.

```{r}
dag1 <- dagify(Y ~ X)
ggdag(dag1) +
  theme_dag()
```

The absense of an arrow between two variables represents the assumption that neither neither variable is causing or caused by the other. In the DAG below, **Z** causes **X** and **Z** causes **Y**, but **X** and **Y** are causally unrelated. 

```{r}
ggdag_confounder_triangle() +
  theme_dag()
```

As we will discuss at length, the absense of an arrow between two nodes on a DAG, doesn't mean that the variables are unassociated in an observational dataset. What if our DAG above represented **sunshine**, **icecream sales** and **sunburn**?

```{r}
ggdag_confounder_triangle(x="icecream sales", y="sunburn", z="sunshine", 
                          use_labels = "label",
                          label_col = "black") +
  theme_dag()
```

We know that icecream sales definitely don't cause sunburn, but you wouldn't be surprised to find an association between icecream sales and incidence of sunburn (a case of correlation not implying causation!). This is a classic example of **confounding bias**. The relationship between icecream sales and sunburn is **confounded** by sunshine, inducing an association between the two variables, even though there is no causal relationship. 

By using expert knowledge to express our qualitative understanding of causal relationships of interest, DAGs help us to distinguish between association and causation.    


### Properties of DAGS
There are some formal requirements for a causal DAG. As indicated by the name, DAGs should be **directed** and **acyclic**.

* **Directed** The relationship between nodes are _directional_. The arrows indicate the direction of causality.

* **Acyclic** DAGs must not have _cycles_ between the nodes, i.e. a variable should not "cause" itself, either directly or through other variables. You can spot a cyclic relationship if it is possible to start at one node and follow the directional arrows to arrive back at the same node. 

Below is an example of a **cyclic** graph. The logic here is tempting: good health will improve educational outcomes, high levels of education will increase income and higher income can improve health. However this results in a cyclic graph: it is posible to start at any node and follow the directional arrows to return to that node, for example the path **health** $\rightarrow$ **education** $\rightarrow$ **income** $\rightarrow$ **health**.

```{r}
dag2 <- dagify(Y ~ X, Z ~ Y, X ~ Z, labels = c(X="health", Y="education", Z="income"))
ggdag(dag2, use_labels = "label") +
  theme_dag()
```

This would suggest that health status causes health status. The graph is cyclic not acyclic, it would not meet the requirements for a DAG. Often, we can untangle cyclic relationships by considering the timing of causes and effects. For example, the relationship between health, education and income could be re-expressed more meaningfully bu considering health status at different points in time.


```{r}

coords <- list(
  x = c(X1 = 0, Y = 2, Z = 4, X2 = 6),
  y = c(X1 = 1, Y = 0, Z = 0, X2 = -1)
)

dag3 <- dagify(Y ~ X1, Z ~ Y, X2 ~ Z,
                labels = c(X1="health in childhood",
                            Y="education",
                            Z="income",
                            X2="health in adulthood"),
                coords = coords)
ggdag(dag3, use_labels = "label") +
  theme_dag()
```

This is a now valid DAG because it is **directional** and **acyclic**. Note the DAG above follows the convention that time moves from left to right. 

### The causal Markov assumption
A further property of a causal DAG is the **causal Markov assumption**. Although this sounds a bit scary, it is quite simple (cf Hernán and Robins 2020 Chapter 6 Technical Point 6.1 for a formal definition). It means that a common cause of any pair of variables on a DAG should also be included in the DAG.

As an example, let's say we are interested in the causal relationship between between aspirin and pulmonary embolism, in a setting where aspirin is more often prescibed to patients with high blood pressure. In this case, high blood pressure is a **common cause** of both aspirin usage and pulmonary embolism: patients with high blood pressure are more likley to be taking aspirin and high blood pressure increases the risk of a pulmonary embolism. To satisfy the causal Markov assumption, blood pressure should be included in the DAG.

``` {r}
ggdag_confounder_triangle(x="aspirin", y="pulmonary embolism", z="blood pressure", 
                          use_labels = "label",
                          label_col = "black",
                          text = FALSE,
                          x_y_associated = TRUE) +
  theme_dag()
```


Now, imagine a different setting where aspirin has been administered as the intervention in a randomised control trial. In this case, patients are randomly assigned, so there is no causal relationship between blood pressure and taking aspirin. Blood pressure still increases the risk of having a pulmonary embolism, but it is no longer a **common cause** of taking aspirin and having a pulmonary embolism so it is not neccessary to include on the DAG. 

```{r}
coords <- list(
  x = c(x = 0, y = 1),
  y = c(x = 1, y = 1)
)

dag4 <- dagify(y ~ x,
                labels = c(x="aspirin", y="pulmonary embolism"),
                coords = coords)

ggdag(dag4, use_labels = "label", text = FALSE) +
  theme_dag()
```

*** 

### Other terminology

Consider the DAG below which an investigator might use when postulating the causal relationship between exercise and melanoma. We will use this DAG to explain some additional useful terminology.

``` {r}
coords <- list(
  x = c(a = 2, b = 1, c = 3, d = 5, e = 7),
  y = c(a = 2, b = 1.1, c = 1, d = 1, e = 1.5)
)

dag5 <- dagify(b ~ a,
               c ~ b,
               d ~ c,
               e ~ d,
               e ~ a,
               labels = c(a="health consciousness",
                          b="exercise",
                          c="sun exposure",
                          d="melanoma",
                          e="screening"),
               exposure = "b",
               outcome = "d",
               coords = coords)

ggdag(dag5, use_labels = "label", text = FALSE) + 
theme_dag()
```

#### Path
A path between two nodes in a DAG is a route that connects the two nodes, visiting no node more than once. The arrows don't have to point in the same direction, but if they do the path is considered a _causal path_. 

Below are examples of paths:

* exercise $\rightarrow$ health consciousness $\rightarrow$ screening 
* health consciousness $\rightarrow$ screening  $\rightarrow$ melanoma 

Below are examples of **causal** paths 

* exercise $\rightarrow$ sun exposure $\rightarrow$ melanoma
* sunexposure $\rightarrow$ melanoma 

#### Parent
The parents for a given node in a DAG are the set of variables with an arrow pointing directly into that node. For example, the parent of **melanoma** is **sun exposure**. 

#### Descendants and Ancestors
The ancestors of a node are any nodes on the causal path leading to that node. The descendants of a node are any nodes that can be reached by following causal paths. 

* The **ancestors** of sun exposure are exercise and health consciousness 
* The **descentants** of health consciousness are exercise, sun exposure, melanoma and screening

#### Mediators
A mediator is a node that lies on the causal path between two other nodes.

* Sun exposure is a mediator on the path between exercise and melanoma 

#### Collider 
A collider is a node with two directional arrows pointing towards it. 

* Screening is a collider on the path health consciousness $\rightarrow$ screening $\leftarrow$ melanoma



child ancester


## Quiz

## D-separation

## Confounding 

## Quiz

## Selection Bias

## Quiz

## Exercise
Draw DAG consistent with two different hypotheses and estimate the corresponding model 





 
 
 
 
 
 
